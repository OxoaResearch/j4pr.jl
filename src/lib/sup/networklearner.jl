##########################
# FunctionCell Interface #
##########################
"""
	networklearner(Adj, fl_train, fl_exec, fr_train, ft_exec [;kwargs])

Constructs an untrained cell that when piped data inside, returns a network learner trained
function cell based on the input data and labels.

# Arguments
  * `x::AbstractMatrix` training data (used by `fl_train`, `fl_exec`; if `use_local_data==true`, it is also used by `fr_train`)
  * `Adj::Vector{AbstractAdjacency}` a vector containing the observation relational structures (adjacency objects)
  * `fl_train` local model training 'function'; can be anything that supports the call `fl_train((X,y))`
  * `fl_exec` local model prediction 'function'; can be anything that supports the call `fl_exec(Ml,X)` where `Ml = fl_train((X,y))`
  * `fr_train` relational model training `function`; can be anything that suports the call `fr_train((Xr,y))` 
  * `fr_exec` relational model prediction `function`; can be anything that suports the call `fr_exec(Mr,Xr)` where `Mr = fr_train((Xr,y))`
  and `Xr` is a dataset of relational variables generated by the relational learner using the results of the local model prediction
  function and adjacency structures.

# Keyword arguments
  * `priors::Vector{Float64}` class priors (if applicable)
  * `learner::Symbol` relational learner (i.e. variable generator); available options `:rn`, `:wrn`, `:bayesrn` and `:cdrn` (default `:wrn`)
  * `inference::Symbol` collective inference method; available options `:rl`, `:ic` and `:gs` (default `:rl`)
  * `normalize::Bool` whether to normalize the relational variables per-observation to the L1 norm (default `true`)
  * `use_local_data::Bool` whether the relational model should use the local data provided (i.e. in `X`) (default `true`)
  * `f_targets::Function` function that extracts targets from estimates generated by the local/relational models 
  (default `f_targets = x->MLDataPattern.targets(indmax,x)`)
  * `obsdim::Int` observation dimension (default `2`)
  * `tol::Float64` maximum admissible mean estimate error for collective inference convergence (default `1e-6`)
  * `κ::Float64` relaxation labeling starting constant, used if `learner == :rl` (default `1.0`)
  * `α::Float64` relaxation labeling decay constant, used if `learner == :rl` (default `0.99`)
  * `maxiter::Int` maximum number of iterations for collective inference (default `100`)
  * `bratio::Float64` percentage of iterations i.e. `maxiter` used for Gibbs sampling burn-in (default `0.1`)

Read the `NetworkLearning.jl` documentation for more information.
"""
networklearner(Adj, fl_train, fl_exec, fr_train, fr_exec; kwargs...) =
	FunctionCell(networklearner, (Adj, fl_train, fl_exec, fr_train, fr_exec), ModelProperties(), "Network Learner"; kwargs...) 



############################
# DataCell/Array Interface #
############################
"""
	networklearner(x, Adj, fl_train, fl_exec, fr_train, ft_exec [;kwargs])

Trains a network learner model that using the data `x`.
"""
# Training
networklearner(x::T where T<:CellDataL, Adj, fl_train, fl_exec, fr_train, fr_exec; kwargs...) = 
	networklearner((getx!(x), gety(x)), Adj, fl_train, fl_exec, fr_train, fr_exec; kwargs...)
networklearner(x::Tuple{T,S} where T<:AbstractVector where S<:AbstractVector, Adj, fl_train, fl_exec, fr_train, fr_exec; kwargs...) =
	networklearner((mat(x[1], LearnBase.ObsDim.Constant{2}()), x[2]), Adj, fl_train, fl_exec, fr_train, fr_exec; kwargs...)
networklearner(x::Tuple{T,S} where T<:AbstractMatrix where S<:AbstractVector, Adj, fl_train, fl_exec, fr_train, fr_exec; kwargs...) = begin
	
	@assert nobs(x[1]) == nobs(x[2]) "[networklearner] Expected $(nobs(x[1])) labels/values, got $(nobs(x[2]))."

	# Transform labels first
	enc = labelencn(x[2])
	yenc = label2ind.(x[2],enc)
	
	# Train model
	nldata = NetworkLearning.fit(NetworkLearning.NetworkLearnerObs, getobs(x[1]), yenc, 
			      		Adj, fl_train, fl_exec, fr_train, fr_exec; kwargs...)

	# Build model properties 
	modelprops = ModelProperties(nvars(x[1]), length(enc.label), enc)
	
	FunctionCell(networklearner, Model(nldata, modelprops), "Network Learner") 

end



# Execution
networklearner(x::T where T<:CellData, model::Model{<:NetworkLearning.AbstractNetworkLearner}) = 
	datacell(networklearner(getx!(x), model), gety(x)) 	

networklearner(x::T where T<:AbstractVector, model::Model{<:NetworkLearning.AbstractNetworkLearner}) = 
	networklearner(mat(x, LearnBase.ObsDim.Constant{2}()), model) 	

networklearner(x::T where T<:AbstractMatrix, model::Model{<:NetworkLearning.AbstractNetworkLearner}) =
	NetworkLearning.predict(model.data, getobs(x))
